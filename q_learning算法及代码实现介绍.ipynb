{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q表格示例\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ef57d8b92cb24cd4ae651892c99a00690a199689e06b4ebb9f742c313fab7f85)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### q learning 算法\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/aa8f36eceb944dc28599c2dae7181a0d0bcee55dc4884d46bf3b4f182e990d96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### q learning 算法 初始化 (与Sarsa一样)\n",
    "``` python\n",
    "    def __init__(self,\n",
    "                     obs_n,\n",
    "                     act_n,\n",
    "                     learning_rate=0.01,\n",
    "                     gamma=0.9,\n",
    "                     e_greed=0.1):\n",
    "            self.act_n = act_n  # 动作维度，有几个动作可选\n",
    "            self.lr = learning_rate  # 学习率\n",
    "            self.gamma = gamma  # reward的衰减率\n",
    "            self.epsilon = e_greed  # 按一定概率随机选动作\n",
    "            self.Q = np.zeros((obs_n, act_n))  # 建立Q表格\n",
    "```\n",
    "其中：<br>\n",
    "学习率 $\\alpha$ 默认为 0.01<br>\n",
    "reward的衰减率（折扣因子）$\\gamma$ 默认为 0.9<br>\n",
    "探索率 $\\varepsilon$ 默认为 0.1 ，即10%的概率进行探索（随机动作）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### q learning 算法 带探索的动作采样 (与Sarsa一样)\n",
    "```Python\n",
    "    # 根据输入观察值，采样输出的动作值，带探索\n",
    "            def sample(self, obs):\n",
    "                if np.random.uniform(0, 1) < (1.0 - self.epsilon):  #根据table的Q值选动作\n",
    "                    action = self.predict(obs)\n",
    "                else:\n",
    "                    action = np.random.choice(self.act_n)  #有一定概率随机探索选取一个动作\n",
    "                return action\n",
    "```\n",
    "其中：<br>\n",
    "使用 90% 的概率，从Q表格中选取当前 **状态$obs$** 的Q值最大的动作 （利用）<br>\n",
    "使用 10% 的概率，从所有可选择的动作中，随机选择动作 （探索）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### q learning 算法 从策略（Q表格）中输出最优动作 (与Sarsa一样)\n",
    "```Python\n",
    "    # 根据输入观察值，预测输出的动作值\n",
    "    def predict(self, obs):\n",
    "        Q_list = self.Q[obs, :]  # 获取状态obs下所有动作的Q值\n",
    "\n",
    "        maxQ = np.max(Q_list)  # 获取最大的Q值（maxQ）\n",
    "\n",
    "        action_list = np.where(Q_list == maxQ)[0]\n",
    "        # 获取最大Q值的索引位置（最优动作）\n",
    "        # maxQ可能对应多个action，意味着最优动作可能不只一个\n",
    "\n",
    "        action = np.random.choice(action_list)\n",
    "        # 选取最优动作，如果在最优动作不只一个的情况下，从中随机选择一个\n",
    "\n",
    "        return action\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### <font color=\"red\">q learning 算法 策略更新</font>\n",
    "```Python\n",
    "    # 学习方法，也就是更新Q-table的方法\n",
    "            def learn(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\" off-policy\n",
    "            obs: 交互前的obs, s_t\n",
    "            action: 本次交互选择的action, a_t\n",
    "            reward: 本次动作获得的奖励r\n",
    "            next_obs: 本次交互后的obs, s_t+1\n",
    "            done: episode是否结束\n",
    "        \"\"\"\n",
    "        predict_Q = self.Q[obs, action]\n",
    "        if done:\n",
    "            target_Q = reward  # 没有下一个状态了\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * np.max(self.Q[next_obs, :])  # Q-learning\n",
    "        self.Q[obs, action] += self.lr * (target_Q - predict_Q)  # 修正q\n",
    "```\n",
    "其中：\n",
    "> target_Q = reward + self.gamma * np.max(self.Q[next_obs, :])\n",
    "\n",
    "公式为：$Q_{target}=R+\\gamma\\cdot max_a Q(S_{t+1}, a)$\n",
    "\n",
    "> self.Q[obs, action] += self.lr * (target_Q - predict_Q)\n",
    "\n",
    "公式为：$Q(S_t,A_t)=Q(S_t,A_t)+\\alpha (Q_{target}-Q(S_t,A_t))$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('shiprl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7941752a6fc2c6f6f5c5d8facdef4400bc87b05a04862ca617946f2aaf624c58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
